// 多头自注意力机制伪代码 MHA
Function MultiHeadCrossAttention(Query_Input,Key_Input,Value_Input,d_k,h):
    #step 1 : 初始化多头的输出容器
    Heads_Output = []

    #step 2 : 遍历每个头
    For head in range(h):
        # 为每个头初始化独立的 W_Q W_K W_V
        W_Q_head,W_K_head,W_V_head = Initialize_Head_Parameters()

        #step 2.1 : 线性变换
        Query = Linear_Transform(Query_Input,W_Q_head)
        Key = Linear_Transform(Key_Input,W_K_head)
        Value = Linear_Transform(Value_Input,W_V_head)
        
        #step 2.2 : 计算注意力分数
        Attention_Scores = Softmax((Query*Transpose(Key)) / sqrt(d_k))

        #step 2.3 : 加权value
        Head_Output = Attention_Scores * Value

        #step 2.4 : 存储头的输出
        Heads_Output.append(Head_Output)
    
    #step 3 : 拼接所有头的输出
    MultiHead_Output = Concat(Heads_Output)

    #step 4 : 投影到输出维度
    Output = Linear_Transform(MultiHead_Output,W_Output)

    Return Output
End Function

# 分组查询注意力 GQA
Function GroupedQueryAttention(Input, h, g, d_k):
    # 说明：
    #   Input: 形状 (B, seq_len, d_model)，B=批大小
    #   h: 总头数
    #   g: 分组数 (g <= h)
    #   d_k: 每个头的维度 (通常 d_model = h * d_k)
    #   其中 heads_per_group = h / g
    #
    #   假设已存在:
    #       WQ[i]:  第 i 个头的线性变换, 形状 (d_model, d_k)
    #       WK[j], WV[j]: 第 j 组的线性变换, 形状 (d_model, d_k)
    #   以及最终输出投影 W_O 形状 (d_model, d_model)
    #
    # 返回: 输出张量 shape (B, seq_len, d_model)

    # Step 1: 计算所有头的 Q
    #   Q_i = LinearTransform(Input, WQ[i])
    #   i=0..(h-1)
    Q_heads = []
    For i in range(h):
        Q_i = MatMul(Input, WQ[i])  # (B, seq_len, d_k)
        Q_heads.append(Q_i)

    # Step 2: 计算所有组的 K, V
    #   K_j = LinearTransform(Input, WK[j])
    #   V_j = LinearTransform(Input, WV[j])
    #   j=0..(g-1)
    K_groups = []
    V_groups = []
    For j in range(g):
        K_j = MatMul(Input, WK[j])  # (B, seq_len, d_k)
        V_j = MatMul(Input, WV[j])
        K_groups.append(K_j)
        V_groups.append(V_j)

    # Step 3: 逐头进行注意力计算
    Heads_Output = array/list of length h
    For i in range(h):
        group_id = i // (h/g)  # 头 i 属于第 group_id 组
        # 取出对应 Q_i, 以及该组共享的 K, V
        Q_i = Q_heads[i]                # (B, seq_len, d_k)
        K_g = K_groups[group_id]        # (B, seq_len, d_k)
        V_g = V_groups[group_id]        # (B, seq_len, d_k)

        # 3.1: 计算注意力得分: Attn = softmax( (Q_i x K_g^T) / sqrt(d_k) )
        #     Q_i shape (B, seqQ, d_k), K_g shape (B, seqK, d_k)
        #     => Attn shape (B, seqQ, seqK)
        AttnScores = BatchMatMul(Q_i, Transpose(K_g)) / sqrt(d_k)
        AttnProbs  = Softmax(AttnScores)  # 在 seqK 维度做 softmax

        # 3.2: 用 AttnProbs 加权 V_g => Head_Output shape (B, seqQ, d_k)
        Head_Output = BatchMatMul(AttnProbs, V_g)

        Heads_Output[i] = Head_Output

    # Step 4: 拼接所有头输出 => (B, seq_len, h*d_k)
    MultiHead_Output = Concat(Heads_Output along last dim)

    # Step 5: 最后输出投影
    Output = MatMul(MultiHead_Output, W_O)  # shape (B, seq_len, d_model)

    return Output
End Function
